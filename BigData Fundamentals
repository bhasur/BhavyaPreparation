A. BigData Basics and Characteristics?
-------------------------------------------

Simply stating, big data is a larger, complex set of data acquired from diverse, new, and old sources of data. 
The data sets are so voluminous that traditional software for data processing cannot manage it. 
Such massive volumes of data are generally used to address problems in business you might not be able to handle.

B. 5 V’s of BigData
----------------------

Volume: the size and amounts of big data that companies manage and analyze.

Velocity: the speed at which companies receive, store and manage data – e.g., the specific number of social media posts or search queries received within a day, 
hour or other unit of time.

Variety: the diversity and range of different data types, including unstructured data, semi-structured data and raw data.

Veracity: the “truth” or accuracy of data and information assets, which often determines executive-level confidence.

Value: the most important “V” from the perspective of the business, the value of big data usually comes from insight discovery and pattern recognition that 
lead to more effective operations, stronger customer relationships and other clear and quantifiable business benefits.

C. Vertical vs Horizontal Scaling
--------------------------------------

Horizontal scaling is adding more machines to deal with increasing requirements. These machines can compute in parallel to improve processing power. 

Vertical scaling is replacing the current machines with more advanced machines to improve throughput.

D. Scaling Up and Scaling Out
-------------------------------
Once a decision has been made for data scaling, the specific scaling approach must be chosen. There are two commonly used types of data scaling, up and out:

Scaling up, or vertical scaling, involves obtaining a faster server with more powerful processors and more memory.
This solution uses less network hardware, and consumes less power; but ultimately, for many platforms may only provide a short-term fix, especially if continued 
growth is expected.

Scaling out, or horizontal scaling, involves adding servers for parallel computing.
The scale out technique is a long-term solution, as more and more servers may be added when needed. 
But going from one monolithic system to this type of cluster may be a difficult, although extremely effective solution.

E. ETL Pipelines
------------------
What is ETL?
Extract, Transform, and Load describes the set of processes to extract data from one system, transform it, and then load it into a target repository.

Extract: the process of pulling data from a source such as an SQL or NoSQL database, an XML file or a cloud platform holding data for systems such as marketing tools,
CRM systems, or transactional systems.

Transform: the process of converting the format or structure of the data set to match the target system.

Load: the process of placing the data set into the target system which can be a database, data warehouse, an application, such as CRM platform or a cloud data 
warehouse, data lake or data lakehouse from providers such as Snowflake, Amazon RedShift, and Google BigQuery.

F: File formats
-----------------
Row based data formats: CSV, JSON, AVRO
column based data formats: ORC, parquet

Q: which is the best file format we can use?
Ans: Storing the data in the data format dependa really how we are accessing the end result

CSV: Good option for compatibility, spreadsheet processing and human readable data. The data must be flat. 
It is not efficient and cannot handle nested data. There may be issues with the separator which can lead to data quality issues. 
Use this format for exploratory analysis, POCs or small data sets.

JSON: Heavily used in APIs. Nested format. 
It is widely adopted and human readable but it can be difficult to read if there are lots of nested fields.
Great for small data sets, landing data or API integration. If possible convert to more efficient format before processing large amounts of data.

Avro: Great for storing row data, very efficient. It has a schema and supports evolution. Great integration with Kafka. 
Supports file splitting. Use it for row level operations or in Kafka. Great to write data, slower to read.

Parquet: Columnar storage. It has schema support. It works very well with Hive and Spark as a way to store columnar data in deep storage that is queried using SQL. 
Because it stores data in columns, query engines will only read files that have the selected columns and not the entire data set as opposed to Avro. 
Use it as a reporting layer.

ORC: Similar to Parquet, it offers better compression. It also provides better schema evolution support as well, but it is less popular.

G: Type of Data
-------------------

Structured: Structured data is the easiest to work with. It is highly organized with dimensions defined by set parameters.

Unstructured: Not all data is as neatly packed and sorted with instructions on how to use as structured data is. 
The consensus is no more than 20% of all data is structured.

Semi-structured: Semi-structured data toes the line between structured and unstructured. 
Most of the time, this translates to unstructured data with metadata attached to it.
This can be inherent data collected, such as time, location, device ID stamp or email address, or it can be a semantic tag attached to the data later.





