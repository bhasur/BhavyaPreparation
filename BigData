What is Big Data?
----------------------
Big data is a combination of structured, semistructured and unstructured data collected by organizations that can be mined for information and 
used in machine learning projects, predictive modeling and other advanced analytics applications.Systems that process and store big data have become a 
common component of data management architectures in organizations, combined with tools that support big data analytics uses


Evolution of Big Data.
-----------------------
The first phase of Big Data evolution consisted of database management and database warehousing.
Modern data analytics later formed as an evolution of the database management system. 
At the time it used techniques like database queries, database processing, and reporting tools.


Why learn Big Data technologies?
--------------------------------
1. Data driven decisions provide a competitive advantage
Many studies have shown that data driven decision are more effective and more efficient than human-generated decisions. 
Big Data allows organisations to detect trends, and spot patterns that can be used for future benefit. 
It can help to detect which customers are likely to buy products, or help to optimise marketing campaigns by identifying which advertisement strategies have 
the highest return on investment. It is easy to see that organisations that ‘know’ more than their competitors, will outperform their peers in the long run.

One of the key business drivers behind Big Data is the ability to start data driven decision making. 
Data driven decision making refers to the practice of basing decisions on the analysis of data rather than purely on intuition. 
Instead of making a decision based on experience, the decision will be based on the best possible scenario. 
With data driven decision making, it is possible to know ‘sooner’ which products and services will successful, providing an opportunity to be first to market. 
Although many companies agree with this in theory, it is estimated that only 11% of organisations think they make significant better use of their data than their peers.
Most organisations still have a long way to go.

2. Big Data provides a spring board for AI
Artificial Intelligence (AI) is one of the most desired areas of expertise in business today. 
What most people not realise, however, is that Big Data provides a ‘foundation’ for organisations that want to start AI projects. 
AI mostly builds on exactly the same techniques and processing capabilities that are required in Big Data organisations. 
Organisations that aspire to start using AI therefore greatly benefit from building a sound and structured Big Data environment first. 
After this has been set up, AI techniques such as cognitive analytics can be taken as next steps.

3. Big Data skills are in high demand
In line with the trends of Big Data in general, the request for skilled Big Data professionals is growing rapidly. 
At the moment, there is more demand than supply, which results in large increases in salaries and payment for people who have the required skill set. 
Major job opportunity platforms such as Indeed or LinkedIn, have been posting and increasing number of job postings looking for Data Analysts or data scientists. 
The demand for Big Data professionals with this particular skill set is on the rise while the supply remains low. 
This creates great job opportunities for individuals within this field.

As the demand steadily increases and the supply remains low, Big Data professionals are getting paid more and more. 
In India, as it stands today, data analytics professionals are paid on average 50% more than their counterparts in other IT based professions. 
This trend is evident across the globe as more and more companies realize just how important these professionals are to the organization.

4. Investments in Big Data keep growing
Many studies and surveys show that investments in Big Data keep growing, year over year. 
The International Data Corporation (IDC) that publishes the Worldwide Semiannual Big Data and Analytics Spending Guide indicates that Big Data-related hardware, 
software, and services are expected to maintain a compound annual growth rate (CAGR) of 11.9% through 2020 when revenues will be more than $210 billion. 
The increase in spending is a sign that Big Data is a trend that will continue in the next years, and that investing time to study Big Data is not just a 
short-term investment.

5. Studying Big Data will broaden your horizon
Last, and maybe most important, studying Big Data is a rewarding and (at times) fun investment of your time. 
The domain of Big Data and data analysis in general is full of puzzles to solve, and will greatly enhance your analytical skills and reasoning. 
The major domains of Big Data involve statistics and problem solving skills. 
Even if you don’t intend to make a career in Big Data, these skills are useful and highly practical on a day-to-day basis.
After you have been spending three to six months of studying Big Data, you will notice that you will start applying ‘Big Data’ techniques into your presentations or 
reports, because they provide a very solid basis for discussion and decision making.
Because in the end, data driven decisions will always outperform intuition based decision. 
So take one hour every day, and start studying the wonderful world of Big Data.

Examples of Big Data
---------------------
Here are some examples of Big Data applications that affect people every day.


Transportation
Advertising and Marketing
Banking and Financial Services
Government
Media and Entertainment
Meteorology
Healthcare
Cybersecurity
Education



Who is using Big Data?
----------------------
Applications of Big Data in the Transportation Industry
Some applications of Big Data by governments, private organizations, and individuals include:

Governments use of Big Data: traffic control, route planning, intelligent transport systems, congestion management (by predicting traffic conditions)
Private-sector use of Big Data in transport: revenue management, technological enhancements, logistics and for competitive advantage 
(by consolidating shipments and optimizing freight movement)
Individual use of Big Data includes route planning to save on fuel and time, for travel arrangements in tourism, etc.



Why is Data so important?
-------------------------
Everyone is in love with data nowadays. Yet data is meaningless. Here’s why:

Data can be irrelevant. If you are working on vaccines, having all data in the world about cabbage production is irrelevant. 
Even if it is relevant for your problem, it is not relevant alone.

Data can be wrong. In fact data is almost always wrong and you need to figure out to what percentage and degree.

Data can be incorrectly interpreted. This happens all the time. 
Any person used to work with data can easily identify incorrect interpretations of data in the media without even looking for it.

Data can be incorrectly used. This comes from the previous point but this one is about action drawn from algorithms.



Characteristics of Big Data:
----------------------------
What are the 5 V's of Big Data?
Big data is a collection of data from many different sources and is often describe by five characteristics: volume, value, variety, velocity, and veracity.

Volume: the size and amounts of big data that companies manage and analyze
Value: the most important “V” from the perspective of the business, the value of big data usually comes from insight discovery and 
pattern recognition that lead to more effective operations, stronger customer relationships and other clear and quantifiable business benefits
Variety: the diversity and range of different data types, including unstructured data, semi-structured data and raw data
Velocity: the speed at which companies receive, store and manage data – e.g., the specific number of social media posts or search queries received within a day, 
hour or other unit of time
Veracity: the “truth” or accuracy of data and information assets, which often determines executive-level confidence
The additional characteristic of variability can also be considered:

Variability: the changing nature of the data companies seek to capture, manage and analyze – e.g., in sentiment or text analytics, changes in the meaning of key words or phrases


Challenges of Big Data
------------------------

The challenges in Big Data are the real implementation hurdles. These require immediate attention and need to be handled because if not handled then the failure of the technology may take place which can also lead to some unpleasant result. Big data challenges include the storing, analyzing the extremely large and fast-growing data.

Some of the Big Data challenges are:

Sharing and Accessing Data:
Perhaps the most frequent challenge in big data efforts is the inaccessibility of data sets from external sources.
Sharing data can cause substantial challenges.
It include the need for inter and intra- institutional legal documents.
Accessing data from public repositories leads to multiple difficulties.
It is necessary for the data to be available in an accurate, complete and timely manner because if data in the companies information system is to be used to make accurate decisions in time then it becomes necessary for data to be available in this manner.


Privacy and Security:
It is another most important challenge with Big Data. This challenge includes sensitive, conceptual, technical as well as legal significance.
Most of the organizations are unable to maintain regular checks due to large amounts of data generation. However, it should be necessary to perform security checks and observation in real time because it is most beneficial.
There is some information of a person which when combined with external large data may lead to some facts of a person which may be secretive and he might not want the owner to know this information about that person.
Some of the organization collects information of the people in order to add value to their business. This is done by making insights into their lives that they’re unaware of.


Analytical Challenges:
There are some huge analytical challenges in big data which arise some main challenges questions like how to deal with a problem if data volume gets too large?
Or how to find out the important data points?
Or how to use data to the best advantage?
These large amount of data on which these type of analysis is to be done can be structured (organized data), semi-structured (Semi-organized data) or unstructured (unorganized data). There are two techniques through which decision making can be done:
Either incorporate massive data volumes in the analysis.
Or determine upfront which Big data is relevant.


Technical challenges:
Quality of data:
When there is a collection of a large amount of data and storage of this data, it comes at a cost. Big companies, business leaders and IT leaders always want large data storage.
For better results and conclusions, Big data rather than having irrelevant data, focuses on quality data storage.
This further arise a question that how it can be ensured that data is relevant, how much data would be enough for decision making and whether the stored data is accurate or not.
Fault tolerance:
Fault tolerance is another technical challenge and fault tolerance computing is extremely hard, involving intricate algorithms.
Nowadays some of the new technologies like cloud computing and big data always intended that whenever the failure occurs the damage done should be within the acceptable threshold that is the whole task should not begin from the scratch.
Scalability:
Big data projects can grow and evolve rapidly. The scalability issue of Big Data has lead towards cloud computing.
It leads to various challenges like how to run and execute various jobs so that goal of each workload can be achieved cost-effectively.
It also requires dealing with the system failures in an efficient manner. This leads to a big question again that what kinds of storage devices are to be used.





Data scale
------------
The four scales of measurement
By understanding the scale of the measurement of their data, data scientists can determine the kind of statistical test to perform.

1. Nominal scale of measurement

The nominal scale of measurement defines the identity property of data. This scale has certain characteristics, but doesn’t have any form of numerical meaning. 
The data can be placed into categories but can’t be multiplied, divided, added or subtracted from one another. 
It’s also not possible to measure the difference between data points.

Examples of nominal data include eye colour and country of birth. Nominal data can be broken down again into three categories:

Nominal with order: Some nominal data can be sub-categorised in order, such as “cold, warm, hot and very hot.”

Nominal without order: Nominal data can also be sub-categorised as nominal without order, such as male and female.

Dichotomous: Dichotomous data is defined by having only two categories or levels, such as “yes’ and ‘no’.

2. Ordinal scale of measurement

The ordinal scale defines data that is placed in a specific order. While each value is ranked, there’s no information that specifies what differentiates the 
categories from each other. These values can’t be added to or subtracted from.

An example of this kind of data would include satisfaction data points in a survey, where ‘one = happy, two = neutral, and three = unhappy.’
Where someone finished in a race also describes ordinal data. While first place, second place or third place shows what order the runners finished in, 
it doesn’t specify how far the first-place finisher was in front of the second-place finisher.

3. Interval scale of measurement

The interval scale contains properties of nominal and ordered data, but the difference between data points can be quantified. 
This type of data shows both the order of the variables and the exact differences between the variables. 
They can be added to or subtracted from each other, but not multiplied or divided. For example, 40 degrees is not 20 degrees multiplied by two.

This scale is also characterised by the fact that the number zero is an existing variable. In the ordinal scale, zero means that the data does not exist. 
In the interval scale, zero has meaning – for example, if you measure degrees, zero has a temperature.

Data points on the interval scale have the same difference between them. The difference on the scale between 10 and 20 degrees is the same between 20 and 30 degrees. 
This scale is used to quantify the difference between variables, whereas the other two scales are used to describe qualitative values only. 
Other examples of interval scales include the year a car was made or the months of the year.

4. Ratio scale of measurement

Ratio scales of measurement include properties from all four scales of measurement. 
The data is nominal and defined by an identity, can be classified in order, contains intervals and can be broken down into exact value. 
Weight, height and distance are all examples of ratio variables. Data in the ratio scale can be added, subtracted, divided and multiplied.

Ratio scales also differ from interval scales in that the scale has a ‘true zero’. 
The number zero means that the data has no value point. 
An example of this is height or weight, as someone cannot be zero centimetres tall or weigh zero kilos – or be negative centimetres or negative kilos. 
Examples of the use of this scale are calculating shares or sales.
Of all types of data on the scales of measurement, data scientists can do the most with ratio data points.


Manage, store and process Big Data
------------------------------------
Apache Hadoop
Apache Hadoop is a set of open-source software for storing, processing, and managing Big Data developed by the Apache Software Foundation in 2006.

As you can see, the Hadoop ecosystem consists of many components. The core three include Hadoop Distributed File System (HDFS), Hadoop MapReduce, and Hadoop YARN.

HDFS splits the data into smaller chunks (each sized 128MB by default) and stores them across different nodes in a cluster.
The MapReduce processing engine is used to process the data stored in Hadoop HDFS in parallel by the means of dividing the task submitted by the user into 
multiple independent subtasks.
YARN (Yet Another Resource Negotiator) is the Hadoop operating system that helps manage and monitor workloads.

Sources of Data flood
----------------------

Among the various natural calamities, flood is considered one of the most catastrophic natural hazards, which has disastrous impact on the socioeconomic 
lifeline of a country. Nowadays, business organizations are using Big Data to improve their strategies and operations for revealing patterns and market trends 
to increase revenues. Eventually, the crisis response teams of a country have turned their interest to explore the potentialities of Big Data in managing disaster 
risks such as flooding. The reason for this is that during flooding, crisis response teams need to take decisions based on the huge amount of incomplete and inaccurate
information, which are mainly coming from three major sources, including people, machines, and organizations. Hence, Big Data technologies can be used to monitor and 
to determine the people exposed to the risks of flooding in real time. This could be achieved by analyzing and processing sensor data streams coming from various 
sources as well as data collected from other sources such as Twitter, Facebook, and satellite and also from disaster organizations of a country by using Big Data 
technologies. Therefore, this chapter explores the challenges, the opportunities, and the methods, required to leverage the potentiality of Big Data to assess and 
predict the risk of flooding.

Exploding data problem
------------------------
The world is currently used to sparing everything without exception in the electronic space. 
Processing power, RAM speeds and hard-disk sizes have expanded to level that has changed our viewpoint towards data and its storage. 
Would you be able to envision having 256 or 512 MB RAM in your PC now?

On the off chance that we comprehend idea of byte, we can envision how data growth has expanded over time and how storage systems handle it.
We know that 1 byte is equivalent to 8 bits and these 8 bits can represent character or expression. 
An archive with huge number of bytes will contain huge number of characters, expressions and spaces etc. 
Similarly, megabyte (MB) is million bytes of information, gigabyte (GB) is billion bytes of information and terabyte (TB) is trillion bytes of information. 
We use these terms while managing data and storage, on our everyday activities.

But it doesn’t end here. Next comes the petabyte, which is quadrillion bytes or million gigabyte. 
Ones after that are Exabyte, Zettabyte, and Yottabyte. Yottabyte is basically trillion terabytes of information. 
There are considerably higher numbers and we’ll stop here now.

As indicated by report by Global Information Enterprise, IDC, in 2009 that aggregate sum of information in world was 800 EB. 
It is expected to ascend to 44 ZB before finish of 2020, i.e., 44 trillion gigabytes. Second explanation they made was that 11 ZB of this information will be put 
away in cloud.


OLTP and OLAP
-------------
OLTP and OLAP: The two terms look similar but refer to different kinds of systems. 
Online transaction processing (OLTP) captures, stores, and processes data from transactions in real time. 
Online analytical processing (OLAP) uses complex queries to analyze aggregated historical data from OLTP systems.

Operational vs Analytical Big Data
----------------------------------

Operational and Analytical Data Systems are both very similar in how they provide information on your organization, company, or non-profit, but the two are very structurally different, and provide different types of insights. That might seem a little confusing, so we're going to break down the differences between the two!


Operational Data Systems
First up, Operational Data is exactly what it sounds like - data that is produced by your organization's day to day operations. 
Things like customer, inventory, and purchase data fall into this category. 
This type of data is pretty straightforward and will generally look the same for most organizations. 
If you want to know the most up to date information on something - you’re using Operational Data! Operational Data Systems support high-volume low-latency access, 
called Online Transactional Processing tables, or OLTP, where you want to create, read, update, or delete one piece of data at a time.

Analytical Data Systems
Analytical Data is a little more complex and will look different for different types of organizations; however, at it's core is an organization's Operational Data. 
Analytical Data is used to make business decisions, as opposed to recording the data from actual operational business processes. 
Examples include grouping customers for market segmentation or changes in purchase volume over time. Every organization will have different questions to answer and 
different decisions to make, so Analytical Data is definitely not one-size-fits-all by any stretch of the imagination! Analytical Data is best stored in a 
Data System designed for heavy aggregation, data mining, and ad hoc queries, called an Online Analytical Processing system, OLAP, or a Data Warehouse! 

To recap, Operational Data Systems, consisting largely of transactional data, are built for quicker updates. 
Analytical Data Systems, which are intended for decision making, are built for more efficient analysis. 
Hopefully now you have a better understanding of the difference between Operational and Analytical Data and their corresponding Data Systems! 
As you can see, both are very important for maintaining and growing an organization, business, or non-profit! 


Possible solutions: scaling up vs. scaling out
---------------------------------------------------------
Once a decision has been made for data scaling, the specific scaling approach must be chosen. There are two commonly used types of data scaling, up and out:

Scaling up, or vertical scaling, involves obtaining a faster server with more powerful processors and more memory. 
This solution uses less network hardware, and consumes less power; but ultimately, for many platforms may only provide a short-term fix, especially if 
continued growth is expected.

Scaling out, or horizontal scaling, involves adding servers for parallel computing. 
The scale out technique is a long-term solution, as more and more servers may be added when needed. 
But going from one monolithic system to this type of cluster may be a difficult, although extremely effective solution.



Challenges of scaling up and scaling out
----------------------------------------
Big data systems are great because they are often easy to scale, but you have to have your plans for keeping track of data and cycling old data out.

That’s why your team has to determine the types of data you’ll collect, how it will be stored, and how it will be used before implementing a data system.

For example, you may want to use a repository in the cloud, but when doing so, it could make more sense to have Parquet files to store like data together.

If you have no method of organizing your data, you could find that it’s much harder to retrieve what you need and that it’s harder to manage your data as you continue 
adding more when your company grows. (As an added benefit, keep in mind that Parquet files generally have a greater performance-to-cost ratio than CSV dumps).










